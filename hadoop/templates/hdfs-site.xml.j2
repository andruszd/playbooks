<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
	    <name>dfs.nameservices</name>
	    <value>{{cluster_name}}</value>
    </property>
    <property>
        <name>dfs.cluster.administrators</name>
        <value>hdfs hadoop-admins</value>
    </property>
    <!-- The minimum number of NameNodes for HA is two. 
    Its suggested to not exceed 5 - with a recommended 3 NameNodes - due to communication overheads. -->
    <property>
        <name>dfs.ha.namenodes.{{cluster_name}}</name>
        <value>{{location}}-{{sys_env}}-hdfsnn01.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsnn02.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsnn03.{{domain_name}}</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.{{cluster_name}}.hdfsnn01</name>
        <value>{{location}}-{{sys_env}}-hdfsnn01.{{domain_name}}:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.{{cluster_name}}.hdfsnn02</name>
        <value>{{location}}-{{sys_env}}-hdfsnn02.{{domain_name}}:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.{{cluster_name}}.hdfsnn03</name>
        <value>{{location}}-{{sys_env}}-hdfsnn03.{{domain_name}}:8020</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.{{cluster_name}}.hdfsnn01</name>
        <value>{{location}}-{{sys_env}}-hdfsnn01.{{domain_name}}:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.{{cluster_name}}.hdfsnn02</name>
        <value>{{location}}-{{sys_env}}-hdfsnn02.{{domain_name}}:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.{{cluster_name}}.hdfsnn03</name>
        <value>{{location}}-{{sys_env}}-hdfsnn03.{{domain_name}}:9870</value>
    </property>
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://{{location}}-{{sys_env}}-hdfsnn01.{{domain_name}}:8485;
        {{location}}-{{sys_env}}-hdfsnn02.{{domain_name}}:8485;
        {{location}}-{{sys_env}}-hdfsnn03.{{domain_name}}:8485/{{cluster_name}}</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.{{cluster_name}}</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>30000</value>
    </property>
    <!-- Remeber to copy the private key to the other nodes(NameNode Only) -->
    <!-- i.e. node1 to all node , node2 to all node , node3 to all node -->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/hdfs/.ssh/id_rsa</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <!--The number of replicas for HDFS this should  equal the number of DataNodes you have-->
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <!--HDFS blocksize of 256MB for large file-systems.-->
        <name>dfs.blocksize</name>
        <value>268435456</value>
    </property>
    <property> 
        <name>dfs.permissions.superusergroup</name> 
        <value>superusergroup</value>
    </property>
    <property>
        <!--More NameNode server threads to handle RPCs from large number of DataNodes.-->
        <name>dfs.namenode.handler.count</name>
        <value>100</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>{{hdfs_namenode_dir}}</value>
    </property>
        <property>
        <name>dfs.datanode.name.dir</name>
        <value>{{hdfs_datanode_data_dir}}</value>
    </property>
    <property>
        <!--List of allowable datanodes-->
        <name>dfs.hosts</name>
        <!-- Does it got to 11-->
        <value>{{location}}-{{sys_env}}-hdfsdn01.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn02.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn03.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn04.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn05.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn06.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn07.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn08.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn09.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn010.{{domain_name}},
        {{location}}-{{sys_env}}-hdfsdn11.{{domain_name}}</value>
    </property>
    <property>
        <name>dfs.hosts.exclude</name>
        <value>{{location}}-{{sys_env}}-hdfsdn12.{{domain_name}}</value>
    </property>
</configuration>





